{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "investing_advice_generation",
      "provenance": [],
      "authorship_tag": "ABX9TyNz8Wde47Px1/3mecLSnymH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roshan-adusumilli/RNN-investing-advice-generation/blob/master/investing_advice_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09wwW4ANYUGn",
        "colab_type": "text"
      },
      "source": [
        "Make the necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I8yEPN9TdDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np \n",
        "import os "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJqLgvpMYZTV",
        "colab_type": "text"
      },
      "source": [
        "Download our data, which is a .txt file of Benjamin Graham's classic investing book *Intelligent Investor*. I removed the preface, index and a few graphs from the file to help our model generate more relevant text. Once we download the file, we take a look at how many total characters are in it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxWkCP2bYXGm",
        "colab_type": "code",
        "outputId": "6dcd505d-d229-432c-bda1-cf18706decd4",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "text = open('The_Intelligent_Investor copy.txt', 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "print ('Length of text: {} characters'.format(len(text)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c79e5166-d61d-4fc9-b484-7f3e3932bdee\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-c79e5166-d61d-4fc9-b484-7f3e3932bdee\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving The_Intelligent_Investor copy.txt to The_Intelligent_Investor copy (2).txt\n",
            "Length of text: 1327032 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFgkUKghZmEA",
        "colab_type": "text"
      },
      "source": [
        "We'll see how many unique characters exist in the file as well as map our characters to integers to later train our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS6gLimgalYc",
        "colab_type": "code",
        "outputId": "7d576159-0be5-45ed-9980-ade5a08e809a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))\n",
        "\n",
        "#Maps characters to ints \n",
        "char2int = {char: num for num, char in enumerate(vocab)}\n",
        "#Maps ints to characters \n",
        "int2char = np.array(vocab)\n",
        "#Intelligent Investor text represented as ints.\n",
        "text_as_int = np.array([char2int[char] for char in text])\n",
        "\n",
        "print('\\n')\n",
        "print(char2int)\n",
        "print('\\n')\n",
        "print(int2char)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "110 unique characters\n",
            "\n",
            "\n",
            "{'\\x01': 0, '\\x02': 1, '\\x03': 2, '\\n': 3, '\\x0c': 4, '\\r': 5, ' ': 6, '!': 7, '\"': 8, '#': 9, '$': 10, '%': 11, '&': 12, '(': 13, ')': 14, '*': 15, '+': 16, ',': 17, '-': 18, '.': 19, '/': 20, '0': 21, '1': 22, '2': 23, '3': 24, '4': 25, '5': 26, '6': 27, '7': 28, '8': 29, '9': 30, ':': 31, ';': 32, '=': 33, '?': 34, 'A': 35, 'B': 36, 'C': 37, 'D': 38, 'E': 39, 'F': 40, 'G': 41, 'H': 42, 'I': 43, 'J': 44, 'K': 45, 'L': 46, 'M': 47, 'N': 48, 'O': 49, 'P': 50, 'Q': 51, 'R': 52, 'S': 53, 'T': 54, 'U': 55, 'V': 56, 'W': 57, 'X': 58, 'Y': 59, 'Z': 60, '[': 61, ']': 62, '_': 63, 'a': 64, 'b': 65, 'c': 66, 'd': 67, 'e': 68, 'f': 69, 'g': 70, 'h': 71, 'i': 72, 'j': 73, 'k': 74, 'l': 75, 'm': 76, 'n': 77, 'o': 78, 'p': 79, 'q': 80, 'r': 81, 's': 82, 't': 83, 'u': 84, 'v': 85, 'w': 86, 'x': 87, 'y': 88, 'z': 89, '~': 90, '¢': 91, '£': 92, 'ç': 93, 'é': 94, 'ê': 95, 'ë': 96, 'î': 97, 'ï': 98, 'ø': 99, '–': 100, '—': 101, '‘': 102, '’': 103, '“': 104, '”': 105, '†': 106, '‡': 107, '•': 108, '⁄': 109}\n",
            "\n",
            "\n",
            "['\\x01' '\\x02' '\\x03' '\\n' '\\x0c' '\\r' ' ' '!' '\"' '#' '$' '%' '&' '(' ')'\n",
            " '*' '+' ',' '-' '.' '/' '0' '1' '2' '3' '4' '5' '6' '7' '8' '9' ':' ';'\n",
            " '=' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P'\n",
            " 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z' '[' ']' '_' 'a' 'b' 'c' 'd' 'e'\n",
            " 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w'\n",
            " 'x' 'y' 'z' '~' '¢' '£' 'ç' 'é' 'ê' 'ë' 'î' 'ï' 'ø' '–' '—' '‘' '’' '“'\n",
            " '”' '†' '‡' '•' '⁄']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYlu_1-3ouvW",
        "colab_type": "text"
      },
      "source": [
        "Our goal for the RNN model is to predict the most likely character after a given sequence of characters. To do this we will break input sequences from the text into an example sequence and target sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA8bJs9go-N8",
        "colab_type": "code",
        "outputId": "606962fd-e417-432e-b584-62917f132b31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#targets are the example sequences, but shifted one character to the right\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create examples and targets sequences \n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_seq(chunk):\n",
        "    example_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return example_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_seq)\n",
        "\n",
        "#look at the first example and target sequence\n",
        "for example_text, target_text in  dataset.take(1):\n",
        "  print ('Example data: ', repr(''.join(int2char[example_text.numpy()])))\n",
        "  print ('Target data:', repr(''.join(int2char[target_text.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example data:  '\\x0c\\r\\n\\x0c                     A Note About Benjamin Graham                        xii\\r\\n\\r\\n  How did Graham'\n",
            "Target data: '\\r\\n\\x0c                     A Note About Benjamin Graham                        xii\\r\\n\\r\\n  How did Graham '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWhup0gS0J3g",
        "colab_type": "text"
      },
      "source": [
        "Our current data isn't the most effective for the model, so we segment the data into batches and then shuffle it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWQmZSsA1Bfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "buffer_size = 10000\n",
        "\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtdU2SPB17rJ",
        "colab_type": "text"
      },
      "source": [
        "Now we'll build the actual model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USjnJEPG2CZA",
        "colab_type": "code",
        "outputId": "c9437e72-a1b8-4b45-afdd-918efd73e237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "#add input layer\n",
        "model.add(tf.keras.layers.Embedding(len(vocab), 256, batch_input_shape=[batch_size, None])) \n",
        "#add RNN layer\n",
        "model.add(tf.keras.layers.GRU(1024, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\n",
        "#add output layer\n",
        "model.add(tf.keras.layers.Dense(len(vocab)))\n",
        "\n",
        "#summary of our model\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           27904     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 109)           111725    \n",
            "=================================================================\n",
            "Total params: 4,077,933\n",
            "Trainable params: 4,077,933\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_g-dOd97MBd",
        "colab_type": "text"
      },
      "source": [
        "Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-fIidmn89_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oDXzcH1-EO1",
        "colab_type": "text"
      },
      "source": [
        "Now we save checkpoints and train our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cog_sW1hAN0W",
        "colab_type": "code",
        "outputId": "740ec4f2-4763-4df7-b921-819f25017d15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "#Make sure the weights are saved\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "history = model.fit(dataset, epochs=30, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 2.9734\n",
            "Epoch 2/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 2.0764\n",
            "Epoch 3/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 1.7660\n",
            "Epoch 4/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 1.5416\n",
            "Epoch 5/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 1.4099\n",
            "Epoch 6/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 1.3258\n",
            "Epoch 7/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 1.2652\n",
            "Epoch 8/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 1.2168\n",
            "Epoch 9/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 1.1745\n",
            "Epoch 10/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 1.1367\n",
            "Epoch 11/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 1.1013\n",
            "Epoch 12/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 1.0662\n",
            "Epoch 13/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 1.0318\n",
            "Epoch 14/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 0.9978\n",
            "Epoch 15/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 0.9638\n",
            "Epoch 16/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 0.9281\n",
            "Epoch 17/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 0.8938\n",
            "Epoch 18/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 0.8584\n",
            "Epoch 19/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 0.8248\n",
            "Epoch 20/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 0.7901\n",
            "Epoch 21/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 0.7586\n",
            "Epoch 22/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 0.7266\n",
            "Epoch 23/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 0.6997\n",
            "Epoch 24/30\n",
            "141/141 [==============================] - 6s 43ms/step - loss: 0.6728\n",
            "Epoch 25/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 0.6473\n",
            "Epoch 26/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 0.6251\n",
            "Epoch 27/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 0.6054\n",
            "Epoch 28/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 0.5904\n",
            "Epoch 29/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 0.5762\n",
            "Epoch 30/30\n",
            "141/141 [==============================] - 6s 42ms/step - loss: 0.5617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKe9THtZMiEr",
        "colab_type": "text"
      },
      "source": [
        "RNNs only accept a fixed batch size and we want to change the batch size to 1 to make the prediction simpler, so we will need to rebuild the model. We previously saved the weights for our batch size of 64, so we can restore them now\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmWSYyqcNa69",
        "colab_type": "code",
        "outputId": "292b9189-1fc4-4f0c-cb04-63bf6004f65e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "#add input layer\n",
        "model.add(tf.keras.layers.Embedding(len(vocab), 256, batch_input_shape=[1, None])) \n",
        "#add RNN layer\n",
        "model.add(tf.keras.layers.GRU(1024, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\n",
        "#add output layer\n",
        "model.add(tf.keras.layers.Dense(len(vocab)))\n",
        "#load weights from previous model\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "#summary of our model\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            27904     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 109)            111725    \n",
            "=================================================================\n",
            "Total params: 4,077,933\n",
            "Trainable params: 4,077,933\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mChOxVIIOWu2",
        "colab_type": "text"
      },
      "source": [
        "Finally we get to the part we are waiting for: the text generation!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtE618itOe_L",
        "colab_type": "code",
        "outputId": "2d23b890-79f0-418e-9362-d111d4c75c80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "#lower temperature gives more predicatable text, higher temperature gives more surprising text\n",
        "#try any temperature in the range of 0.1 to 1\n",
        "def generate_text(model, start_string, temperature):\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2int[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  \n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      \n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(int2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n",
        "\n",
        "print(generate_text(model, start_string=\"Advice: \", temperature=.5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Advice: “In its businesses and more on “ashight”? You can use “portfolio trackers” at websites like www.morningstar.com or conservative\r\n",
            "companies, including Barrard & Poor’s—are investing on the other hand, if\r\n",
            "his company’s earnings as if it were the best to use the rest of your tax\r\n",
            "swings down to the conservative investor in companies that are seriously only a minimum\r\n",
            "of 40% fully offsets a predection of a company’s capital in a separate company. The first is the perfect record\r\n",
            "of high-grade bonds and preferred stocks to companies that are really disclosed at the time of convertible\r\n",
            "bonds, and safety of price discounts and residential properties.1\r\n",
            "    What about Exodus the business? Graham wouldn’t have to be properly bear-\r\n",
            "ing about the future. If the analyst was above are not actively sold in 1970, the\r\n",
            "advantage of the best professional analyst was able to then be spared the market as a\r\n",
            "whole has also varied from one date to another. Figures\r\n",
            "on this point for lower coupons. The “\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}